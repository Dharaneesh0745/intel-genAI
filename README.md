Introduction to GenAI and Simple LLM Inference on CPU and finetuning of LLM Model to create a Custom Chatbot.

In the rapidly evolving field of artificial intelligence, Generative AI (GenAI) stands at the forefront, enabling machines to understand, interpret, and generate human-like text. Leveraging the power of language models, particularly the LLaMA-2 model, represents a significant advancement. This model, integrated into our server environment using server.py, harnesses the computational capabilities of CPUs to perform inference tasks efficiently.

Finetuning of LLM Model to Create a Custom Chatbot

To personalize and enhance the capabilities of our LLM model, we engage in finetuning. This process involves training the model on specific datasets relevant to our application, thereby tailoring its responses and behavior to meet our project's requirements. By utilizing server.py, we establish an API route that connects seamlessly with our front-end interface built on Next.js, utilizing TypeScript for robust and scalable development.


To run this project.

      yarn
      yarn dev
      python server.py

Run on local server.

      http://localhost:3000/

Live project on the below link.

      https://intel-genai.vercel.app/
                    
